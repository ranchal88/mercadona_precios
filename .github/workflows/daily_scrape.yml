name: Daily Mercadona Scrape

on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:

permissions:
  contents: write
  actions: read

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install playwright

      - name: Install Playwright browsers
        run: |
          playwright install chromium

      - name: Restore X cookies
        run: |
          echo '${{ secrets.X_COOKIES }}' > cookies.json

      - name: Run scraper
        run: python scripts/mercadona_ccaa_daily.py

      - name: Generate tweet TXT
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: python scripts/generate_and_tweet.py

      - name: Package data
        run: |
          DATE=$(date -u +%F)
          echo "DATE=$DATE" >> $GITHUB_ENV

          if [ -d "data" ]; then
            zip -r "mercadona_ccaa_${DATE}.zip" data output
          else
            echo "❌ data/ directory not found. Scraper probably failed."
            exit 1
          fi

      - name: Create GitHub Release + upload zip
        uses: softprops/action-gh-release@v2
        with:
          tag_name: "data-${{ env.DATE }}"
          name: "Mercadona CCAA - ${{ env.DATE }}"
          body: |
            Datos diarios por CCAA (CSV dentro del ZIP).
            Generado automáticamente por GitHub Actions.
          files: |
            mercadona_ccaa_${{ env.DATE }}.zip
